{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a424b415-b8ef-47b3-84ee-7c40332e01ef",
        "outputId": "4d4c4c36-6a37-4045-889c-a60c4f076fef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.7.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.6 MB 6.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.7.2\n"
          ]
        }
      ],
      "source": [
        "!pip install faiss-cpu --no-cache"
      ],
      "id": "a424b415-b8ef-47b3-84ee-7c40332e01ef"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "837dcaa9-cc0b-4345-bbee-3a09683decf9"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import faiss\n",
        "import numpy as np"
      ],
      "id": "837dcaa9-cc0b-4345-bbee-3a09683decf9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41f1f7cb-9441-4fe1-962e-9e4cb3e3c192"
      },
      "source": [
        "# **Welcome to week 3 project!**\n",
        "\n",
        "Congratulations on making it to week 3! 👏 In the first week of this course, we covered the basics of how to design personalized recommendation systems. We then provided some system design examples for large scale recommenders from corporations like Spotify and YouTube, as well as techniques for candidate generation, specifically the two-tower model being used at Twitter and Pinterest.\n",
        "\n",
        "Last week, we covered details of ML approaches for recommendations: including multi-task recommenders and contextual bandits.\n",
        "\n",
        "In week 3, we covered various techniques for learning user representations.\n",
        "\n",
        "In this week's project, we will touch upon two key aspects related to representations:\n",
        "1. How do we query large amount of vectors in efficient time.\n",
        "2. How can we infer various user representations and see what their impact is on downstream task.\n",
        "\n",
        "Lets begin with Part A, which tells us how we could handle a large number of candidate items or user representations in an efficient manner. \n"
      ],
      "id": "41f1f7cb-9441-4fe1-962e-9e4cb3e3c192"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0e01328-6834-4b11-85c1-03867e1d4860"
      },
      "source": [
        "# Part A: Approximate nearest neighbor search\n",
        "\n",
        "Often we are interested in finding nearest neighbors in a large space of vectors. To store embeddings for 400 million users and over 100 million items and querying them in real time is a challenging task. This is where approximate nearest neighbor approaches step in to help. Annoy, Faiss, ScaNN are typical libraries that are used for efficient vector similarity search at scale. They implement algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM.\n",
        "\n",
        "In the first part of this week's project, we will simulate embeddings of 1 million items and try to find k-nearest neighbours for an item of interest. We will implement a vanilla search function to fetch the top-k nearest neighbors and estimate the time it takes for us to do so. We will then compare this with FAISS -- Facebook's nearest neighbour search library, and compare the time it takes for us to get nearest neighbours from FAISS versus our own implementation."
      ],
      "id": "d0e01328-6834-4b11-85c1-03867e1d4860"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fa2b438-be53-4f39-b990-859046a4a560"
      },
      "source": [
        "Lets first generate a simulated dataset of embeddings of 1 million items."
      ],
      "id": "3fa2b438-be53-4f39-b990-859046a4a560"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8c36fc8b-a032-4b48-bbeb-3bff46624615"
      },
      "outputs": [],
      "source": [
        "d = 64                           # dimension\n",
        "nb = 1000000                     # database size\n",
        "nq = 10000                       # nb of queries\n",
        "np.random.seed(1234)             # make reproducible\n",
        "xb = np.random.random((nb, d)).astype('float32')\n",
        "xq = np.random.random((nq, d)).astype('float32')"
      ],
      "id": "8c36fc8b-a032-4b48-bbeb-3bff46624615"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "142c00a5-f36b-4a5f-9e27-4a3e38b467d7"
      },
      "source": [
        "Now that we have these items, lets take up the goal of finding the top-5 items closest to this specific item. Your goal is to implement your function to estimate the top-5 items and print the average distance of these top 5 items to the query item."
      ],
      "id": "142c00a5-f36b-4a5f-9e27-4a3e38b467d7"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "b2dd70b9-8d2c-4134-b866-ab09af43a47d"
      },
      "outputs": [],
      "source": [
        "k=4\n",
        "query_vector = xb[2:3]"
      ],
      "id": "b2dd70b9-8d2c-4134-b866-ab09af43a47d"
    },
    {
      "cell_type": "code",
      "source": [
        "query_vector.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhFdUXceK3RP",
        "outputId": "716ab851-a71c-4090-edc4-5bf626be26cc"
      },
      "id": "IhFdUXceK3RP",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 64)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import vdot\n",
        "from numpy.linalg import norm\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "def cos_sim(a, b)-> float:\n",
        "  \"\"\"\n",
        "  cosine similarity \n",
        "  \"\"\"\n",
        "  a = a.reshape(-1)\n",
        "  b = b.reshape(-1)\n",
        "  return vdot(a, b)/(norm(a)*norm(b))\n",
        "\n",
        "def squared_l2(a,b) -> float:\n",
        "  a = a.reshape(-1)\n",
        "  b = b.reshape(-1)\n",
        "  # subtracting vector\n",
        "  temp = a - b \n",
        "\n",
        "  return np.dot(temp.T, temp)\n",
        "\n",
        "a,b = xb[2:3] , xb[3:4]\n",
        "print(cos_sim(a,b))\n",
        "print(cosine_similarity(a, b)[0][0])\n",
        "\n",
        "print(squared_l2(a,b))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGs97OP_LOJp",
        "outputId": "a1c5b469-2684-4c4a-9875-b44a000f9747"
      },
      "id": "AGs97OP_LOJp",
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.72764724\n",
            "0.72764707\n",
            "12.252186\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "CnseQjeYeb4c"
      },
      "id": "CnseQjeYeb4c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### floating point precision error....  less than 10e-6..."
      ],
      "metadata": {
        "id": "K_v9YvV2LVLm"
      },
      "id": "K_v9YvV2LVLm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "84a94ee2-3131-44c5-ba95-ec67aa675048"
      },
      "outputs": [],
      "source": [
        "def find_top_k_nn(query_vector,k):\n",
        "    \"\"\"\n",
        "    in this function, implement your definition of top-k nearest neighbours, and return the distances\n",
        "    and indices of the these top-k items.\n",
        "    \"\"\"  \n",
        "    # Step1: cal nn\n",
        "    idx_dist_list = []\n",
        "    for idx, test_vector in enumerate(xb):\n",
        "      idx_dist_list.append((idx, squared_l2(query_vector, test_vector)))\n",
        "\n",
        "    ##Step2: sort the list\n",
        "    idx_dist_list_top_k = sorted(idx_dist_list, key=lambda tuple_x : tuple_x[1])[:k]\n",
        "\n",
        "    ##Step3: collect list of tuples as separate lists \n",
        "    I, D = list(zip(*idx_dist_list_top_k))\n",
        "\n",
        "    ##Step4: invert and return as numpy arrays\n",
        "    return np.array(D), np.array(I)"
      ],
      "id": "84a94ee2-3131-44c5-ba95-ec67aa675048"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8a895ce-4f0b-41a5-8243-90f18bff8d7b"
      },
      "source": [
        "With your top-k NN function implemented, call this function to get the top-k nearest neighbor items for the query_vector and print the average distance. Also, print the time it takes to run this function:"
      ],
      "id": "b8a895ce-4f0b-41a5-8243-90f18bff8d7b"
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0df9e9f8-a854-4896-9ebd-fd74bad21690",
        "outputId": "24a2f98e-2a17-463a-e748-81f0283ad0e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "distances from the k nearest neighbor fetched: [0.        4.2204943 4.385453  4.693385 ]\n",
            "indices from the k nearest neighbor fetched: [     2 379284 539651 400245]\n",
            "average distance of the k- nearest neighbors fetched:  3.3248332\n",
            "CPU times: user 4.17 s, sys: 83.7 ms, total: 4.26 s\n",
            "Wall time: 4.24 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "D, I = find_top_k_nn(query_vector, k)\n",
        "print(\"distances from the k nearest neighbor fetched:\",D)\n",
        "print(\"indices from the k nearest neighbor fetched:\",I)\n",
        "print(\"average distance of the k- nearest neighbors fetched: \",D.mean())"
      ],
      "id": "0df9e9f8-a854-4896-9ebd-fd74bad21690"
    },
    {
      "cell_type": "code",
      "source": [
        "from heapq import heapify, heappush, heappop\n",
        "\n",
        "def find_top_k_nn_max_heap(query_vector,k):\n",
        "    \"\"\"\n",
        "    in this function, implement your definition of top-k nearest neighbours, and return the distances\n",
        "    and indices of the these top-k items.\n",
        "    \"\"\"  \n",
        "    \n",
        "  # priority as the first element.  --- also negate the priority value for implementing maxPriorityQueue  \n",
        "    idx_dist_topk_heap = []\n",
        "    for idx, test_vector in enumerate(xb[:k]):\n",
        "      idx_dist_topk_heap.append((-squared_l2(query_vector, test_vector), idx))\n",
        "    \n",
        "    heapify(idx_dist_topk_heap)  \n",
        "\n",
        "    for idx, test_vector in enumerate(xb[k:]):\n",
        "      heappush(idx_dist_topk_heap, (-squared_l2(query_vector, test_vector), idx))\n",
        "      heappop(idx_dist_topk_heap)  \n",
        "\n",
        "    ##Step3: collect list of tuples as separate lists \n",
        "    D, I = list(zip(*idx_dist_topk_heap))\n",
        "\n",
        "    ##Step4: invert and return as numpy arrays\n",
        "    return np.array(D)*-1, np.array(I)"
      ],
      "metadata": {
        "id": "S5YeKmrbyGDa"
      },
      "id": "S5YeKmrbyGDa",
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "D, I = find_top_k_nn_max_heap(query_vector, k)\n",
        "print(\"distances from the k nearest neighbor fetched:\",D)\n",
        "print(\"indices from the k nearest neighbor fetched:\",I)\n",
        "print(\"average distance of the k- nearest neighbors fetched: \",D.mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrxV9X9syLyx",
        "outputId": "92d88ec7-61d9-48ab-8810-219400d1a5e5"
      },
      "id": "YrxV9X9syLyx",
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "distances from the k nearest neighbor fetched: [4.693385  4.385453  0.        4.2204943]\n",
            "indices from the k nearest neighbor fetched: [400241 539647      2 379280]\n",
            "average distance of the k- nearest neighbors fetched:  3.3248332\n",
            "CPU times: user 4.04 s, sys: 182 ms, total: 4.23 s\n",
            "Wall time: 4.02 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd4452f5-ce17-43df-95f2-e9b58e8cad5e"
      },
      "source": [
        "Now lets switch to using Faiss https://github.com/facebookresearch/faiss\n",
        "\n",
        "Faiss is a library for efficient similarity search and clustering of dense vectors. It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM. "
      ],
      "id": "bd4452f5-ce17-43df-95f2-e9b58e8cad5e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6427be9-53b2-446c-851b-b2f6b7db9fa6"
      },
      "source": [
        "### Similarity search in Faiss\n",
        "\n",
        "Given a set of vectors x_i in dimension d, Faiss builds a data structure in RAM. After the structure is constructed, when given a new vector x in dimension d it performs efficiently the operation:\n",
        "\n",
        "$i = argmin_i ||x - x_i||$\n",
        "\n",
        "where ||.|| is the Euclidean distance (L2).\n",
        "\n",
        "In Faiss terms, the data structure is an index, an object that has an add method to add x_i vectors. Note that the x_i's are assumed to be fixed. Computing the argmin is the search operation on the index.\n",
        "\n",
        "### Indexes used by Faiss\n",
        "\n",
        "1. The inverted file from “Video google: A text retrieval approach to object matching in videos.”, Sivic & Zisserman, ICCV 2003. This is the key to non-exhaustive search in large datasets. Otherwise all searches would need to scan all elements in the index, which is prohibitive even if the operation to apply for each element is fast\n",
        "\n",
        "\n",
        "2. The product quantization (PQ) method from “Product quantization for nearest neighbor search”, Jégou & al., PAMI 2011. This can be seen as a lossy compression technique for high-dimensional vectors, that allows relatively accurate reconstructions and distance computations in the compressed domain.\n",
        "\n",
        "\n",
        "3. The three-level quantization (IVFADC-R aka IndexIVFPQR) method from \"Searching in one billion vectors: re-rank with source coding\", Tavenard & al., ICASSP'11."
      ],
      "id": "b6427be9-53b2-446c-851b-b2f6b7db9fa6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e63810d-fd22-49ca-b4bd-423ebf348c5f"
      },
      "source": [
        "We will implement these three indexes from faiss and use each of these three to search the index, and get the top-k nearest neighbour vectors, and estimate the average distance.\n",
        "\n",
        "Lets first construct the three indexes: index1, index2, index3 based on Flat index, Inverted index and product quantization techniques:"
      ],
      "id": "3e63810d-fd22-49ca-b4bd-423ebf348c5f"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76487ec4-a020-4ccd-aa33-f6ef25869048",
        "outputId": "89716253-5384-4c99-fc7e-fc47b1c176a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total number of vectors indexed =  1000000\n",
            "CPU times: user 91 ms, sys: 0 ns, total: 91 ms\n",
            "Wall time: 90.5 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "index1 = faiss.IndexFlatL2(d)   # build the index\n",
        "index1.add(xb)                  # add vectors to the index\n",
        "print(\"total number of vectors indexed = \",index1.ntotal)"
      ],
      "id": "76487ec4-a020-4ccd-aa33-f6ef25869048"
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55447035-57e2-4938-837b-9bf20bf66ff6",
        "outputId": "e7d0ce68-853d-44ef-c2ac-5267eabd86e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total number of vectors indexed =  1000000\n",
            "CPU times: user 2.54 s, sys: 155 ms, total: 2.7 s\n",
            "Wall time: 739 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "nlist = 100\n",
        "quantizer = faiss.IndexFlatL2(d)  # the other index\n",
        "index2 = faiss.IndexIVFFlat(quantizer, d, nlist, faiss.METRIC_L2)\n",
        "index2.train(xb)\n",
        "index2.add(xb)\n",
        "print(\"total number of vectors indexed = \",index2.ntotal)"
      ],
      "id": "55447035-57e2-4938-837b-9bf20bf66ff6"
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f04ebe3c-705d-49b2-aa7b-3602c111c754",
        "outputId": "f0b7c1c7-5f76-4303-c9a2-d1a86d20e9d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total number of vectors indexed =  1000000\n",
            "CPU times: user 46 s, sys: 45.3 ms, total: 46.1 s\n",
            "Wall time: 11.7 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "nlist = 100\n",
        "m = 8\n",
        "k = 4\n",
        "quantizer = faiss.IndexFlatL2(d)  # this remains the same\n",
        "index3 = faiss.IndexIVFPQ(quantizer, d, nlist, m, 8)\n",
        "                                  # 8 specifies that each sub-vector is encoded as 8 bits\n",
        "index3.train(xb)\n",
        "index3.add(xb)\n",
        "print(\"total number of vectors indexed = \",index3.ntotal)"
      ],
      "id": "f04ebe3c-705d-49b2-aa7b-3602c111c754"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "047a6682-f771-4672-8998-307f4cfad5f5"
      },
      "source": [
        "Now that we have these three indexes, let us query these to fetch the top-k nearest neghbour for our query_vector and compute the average distance we obtain for each.\n",
        "\n",
        "We will also time these commands, to find out the trade-off between accuracy and latency."
      ],
      "id": "047a6682-f771-4672-8998-307f4cfad5f5"
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2e54dc7d-5d15-47b2-85e4-c583f92d52a8",
        "outputId": "d2e216c3-18f9-44fd-cc5d-012aa77d0355"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "distances from the k nearest neighbor fetched: [[0.        4.2204943 4.3854527 4.6933837]]\n",
            "indices from the k nearest neighbor fetched: [[     2 379284 539651 400245]]\n",
            "average distance of the k- nearest neighbors fetched:  3.324833\n",
            "CPU times: user 59.1 ms, sys: 0 ns, total: 59.1 ms\n",
            "Wall time: 58 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "D, I = index1.search(query_vector, k)\n",
        "print(\"distances from the k nearest neighbor fetched:\", D)\n",
        "print(\"indices from the k nearest neighbor fetched:\", I)\n",
        "print(\"average distance of the k- nearest neighbors fetched: \", D.mean())"
      ],
      "id": "2e54dc7d-5d15-47b2-85e4-c583f92d52a8"
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "be8f1b17-6108-41fd-814c-caa996fb731d",
        "outputId": "c1ac9cb8-5642-4cbb-e312-cf3c5b8f7c58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "distances from the k nearest neighbor fetched: [[0.        5.0635023 5.4133463 5.642405 ]]\n",
            "indices from the k nearest neighbor fetched: [[     2 859123 177280  74082]]\n",
            "average distance of the k- nearest neighbors fetched:  4.0298133\n",
            "CPU times: user 4.29 ms, sys: 992 µs, total: 5.28 ms\n",
            "Wall time: 4.08 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "D, I = index2.search(query_vector, k)\n",
        "print(\"distances from the k nearest neighbor fetched:\", D)\n",
        "print(\"indices from the k nearest neighbor fetched:\", I)\n",
        "print(\"average distance of the k- nearest neighbors fetched: \", D.mean())"
      ],
      "id": "be8f1b17-6108-41fd-814c-caa996fb731d"
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b230291-517c-49c5-a634-f9d4d331e241",
        "outputId": "f43f89d1-e733-4cad-9878-59c03f2d4171"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "distances from the k nearest neighbor fetched: [[1.1111705 5.027693  5.1296997 5.1854224]]\n",
            "indices from the k nearest neighbor fetched: [[     2 351653 703885 841943]]\n",
            "average distance of the k- nearest neighbors fetched:  4.1134963\n",
            "CPU times: user 4.06 ms, sys: 1.05 ms, total: 5.11 ms\n",
            "Wall time: 4.05 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "D, I = index3.search(query_vector, k)\n",
        "print(\"distances from the k nearest neighbor fetched:\", D)\n",
        "print(\"indices from the k nearest neighbor fetched:\", I)\n",
        "print(\"average distance of the k- nearest neighbors fetched: \", D.mean())"
      ],
      "id": "2b230291-517c-49c5-a634-f9d4d331e241"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2c902e3-c9bf-408e-9052-11af5a3b069f"
      },
      "source": [
        "Running all these, we observe that the **product quantization(PQ) based index** is an order of magnitude faster than the inverted index. In terms of accuracy, if we **assume** that the **lower the distance(mean@k) the more accurate the result**, FlatIndex gives us the least distance."
      ],
      "id": "c2c902e3-c9bf-408e-9052-11af5a3b069f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "044a818f-19ab-4c26-8532-922c9aa1d673"
      },
      "source": [
        "### Goal 1 for this week: Implement your k-NN function and time it\n",
        "\n",
        "The main goal for this part of the project is to implement your vanilla nearest neighbor function and fetch the closest k nearest neighbours to the query vector. Important to note that your implementation will give an exact result, i.e., your implementation will find the exact closest k vectors that will give the minimum distance to the query_vector.\n",
        "\n",
        "Please compile the results in a table, and compare the average distance obtained and the time it took to query the 1 million vectors. A nice 2D plot would also give you a good idea of the speed-accuracy trade-off involved."
      ],
      "id": "044a818f-19ab-4c26-8532-922c9aa1d673"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- Wall clock:\n",
        "  - how much time has passed, as if you were looking at the clock on your wall. \n",
        "- CPU time:\n",
        "  - how many seconds the CPU was busy.\n",
        "\n",
        "```\n",
        "wall time:  <-------------->   16\n",
        "    CPU 1:  ## #############\n",
        "    CPU 2:      ##########\n",
        " CPU time:                     25\n",
        "``` \n",
        "- in a distributed sys -- its good to monitor wall time to see gains from parallelization\n",
        "\n"
      ],
      "metadata": {
        "id": "pP-tIpthjJ_H"
      },
      "id": "pP-tIpthjJ_H"
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "DPMojA-cmjPW"
      },
      "id": "DPMojA-cmjPW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "2e69dd09-6a21-49a8-8ca8-4d5164a7e674",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "outputId": "2c984228-56e1-414f-c808-6c35e6d7eabc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                  Average distance  CPU time  Wall time ms\n",
              "Vanila                                    3.324833   4170.00       4240.00\n",
              "FAISS flat index                          3.324833     59.10         58.00\n",
              "FAIIS inverted index                      4.029813      4.29          4.08\n",
              "FAISS product quantizer encoding          4.113496      4.06          4.05"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4b0c3698-b32a-41ec-87aa-fed704911459\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Average distance</th>\n",
              "      <th>CPU time</th>\n",
              "      <th>Wall time ms</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Vanila</th>\n",
              "      <td>3.324833</td>\n",
              "      <td>4170.00</td>\n",
              "      <td>4240.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>FAISS flat index</th>\n",
              "      <td>3.324833</td>\n",
              "      <td>59.10</td>\n",
              "      <td>58.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>FAIIS inverted index</th>\n",
              "      <td>4.029813</td>\n",
              "      <td>4.29</td>\n",
              "      <td>4.08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>FAISS product quantizer encoding</th>\n",
              "      <td>4.113496</td>\n",
              "      <td>4.06</td>\n",
              "      <td>4.05</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4b0c3698-b32a-41ec-87aa-fed704911459')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4b0c3698-b32a-41ec-87aa-fed704911459 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4b0c3698-b32a-41ec-87aa-fed704911459');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ],
      "source": [
        "results_table = pd.DataFrame({\n",
        "    \"Average distance\": [3.3248332, 3.324833, 4.0298133, 4.1134963],\n",
        "    \"CPU time\": [4170 , 59.1, 4.29, 4.06],\n",
        "    \"Wall time ms\": [4240, 58, 4.08, 4.05]\n",
        "}, index= [\"Vanila\", \"FAISS flat index\", \"FAIIS inverted index\", \"FAISS product quantizer encoding\"]\n",
        ")\n",
        "\n",
        "results_table"
      ],
      "id": "2e69dd09-6a21-49a8-8ca8-4d5164a7e674"
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "sns.set_style(\"dark\")\n",
        "results_table['Wall time ms'].plot(kind=\"barh\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "ortPccZ5pq7t",
        "outputId": "a0aae471-e9f8-4a9c-f4f3-ce015b7bfe9b"
      },
      "id": "ortPccZ5pq7t",
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f9416102ad0>"
            ]
          },
          "metadata": {},
          "execution_count": 100
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://drive.google.com/file/d/1xU2_7ei7q_n1npgOJyYwJuNjmtntW3Ye/view?usp=sharing\n",
        "!gdown https://drive.google.com/uc?id=1xU2_7ei7q_n1npgOJyYwJuNjmtntW3Ye\n",
        "!unzip hmdata.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikCjkfzM9Ua7",
        "outputId": "d2aab8f6-e68f-40d5-c22c-749790f35ff7"
      },
      "id": "ikCjkfzM9Ua7",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1xU2_7ei7q_n1npgOJyYwJuNjmtntW3Ye\n",
            "To: /content/hmdata.zip\n",
            "100% 773M/773M [00:09<00:00, 82.7MB/s]\n",
            "Archive:  hmdata.zip\n",
            "   creating: hmdata/\n",
            "  inflating: hmdata/customers.csv.zip  \n",
            "  inflating: __MACOSX/hmdata/._customers.csv.zip  \n",
            "  inflating: hmdata/articles.csv.zip  \n",
            "  inflating: __MACOSX/hmdata/._articles.csv.zip  \n",
            "  inflating: hmdata/transactions_train.csv.zip  \n",
            "  inflating: __MACOSX/hmdata/._transactions_train.csv.zip  \n",
            "  inflating: hmdata/sample_submission.csv.zip  \n",
            "  inflating: __MACOSX/hmdata/._sample_submission.csv.zip  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e127987e-44f9-43b3-9219-1036edd0d14c"
      },
      "source": [
        "# Part B: User representations"
      ],
      "id": "e127987e-44f9-43b3-9219-1036edd0d14c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13c18e33-3a62-4afe-95af-9d33ef87917c"
      },
      "source": [
        "In the second part of this week's project, we wish to understand few ways of estimating user representations, and how it impacts the performance of downstream tasks.\n",
        "\n",
        "To this end, we will work on top of our H&M dataset, and develop a few different ways of representing users.\n",
        "\n",
        "The broader framework here will be -- we fix the article representations, and fix the downstream task, and then vary the user representations and see how the performance of the downstream task changes based on different user representation techniques.\n"
      ],
      "id": "13c18e33-3a62-4afe-95af-9d33ef87917c"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd824205-1569-4b54-8c48-fda93a59053d",
        "outputId": "745ca00a-6abf-44d9-9888-09652198d04d",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.7/dist-packages (2.2.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from lightgbm) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from lightgbm) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from lightgbm) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->lightgbm) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->lightgbm) (3.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install lightgbm"
      ],
      "id": "dd824205-1569-4b54-8c48-fda93a59053d"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "251abdd1-1cd4-41f9-af70-23d22a1aa455"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import random\n",
        "import gc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import lightgbm as lgb\n",
        "import datetime\n",
        "import itertools\n",
        "import os\n",
        "from contextlib import redirect_stdout\n",
        "from tqdm.notebook import tqdm"
      ],
      "id": "251abdd1-1cd4-41f9-af70-23d22a1aa455"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "219f3784-220e-4889-ae88-09c80f556479"
      },
      "source": [
        "While we have used neural models so far, lets try a tree based model for this task. We use LightGBM library to train the main model. Lets set up few parameters for the lightgbm model, and specify some additional parameters:"
      ],
      "id": "219f3784-220e-4889-ae88-09c80f556479"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "21e76b18-8d9e-4a1b-bf84-a929c8f16c6f"
      },
      "outputs": [],
      "source": [
        "rand = 64\n",
        "lgb_params = {\n",
        "    \"objective\": \"binary\",\n",
        "    \"boosting\": \"gbdt\",\n",
        "    \"max_depth\": -1,\n",
        "    \"num_leaves\": 40,\n",
        "    \"subsample\": 0.8,\n",
        "    \"subsample_freq\": 1,\n",
        "    \"bagging_seed\": rand,\n",
        "    \"learning_rate\": 0.05,\n",
        "    \"feature_fraction\": 0.6,\n",
        "    \"min_data_in_leaf\": 100,\n",
        "    \"lambda_l1\": 0,\n",
        "    \"lambda_l2\": 0,\n",
        "    \"random_state\": rand,\n",
        "    \"metric\": \"auc\",#\"binary_logloss\",\n",
        "    \"verbose\": -1\n",
        "}\n",
        "\n",
        "tran_dtypes = {\"t_dat\":\"str\",\n",
        "               \"customer_id\":\"str\",\n",
        "               \"article_id\":\"int\",\n",
        "               \"product_code\":\"int\",\n",
        "               \"price\":\"float\",\n",
        "               \"sales_channel_id\":\"int\"}\n",
        "art_dtypes = {\"article_id\":\"int\",\n",
        "              \"product_code\":\"int\",\n",
        "              \"product_type_no\":\"int\",\n",
        "              \"graphical_appearance_no\":\"int\",\n",
        "              \"colour_group_code\":\"int\",\n",
        "              \"department_no\":\"int\",\n",
        "              \"index_code\":\"str\",\n",
        "              \"index_group_no\":\"int\",\n",
        "              \"section_no\":\"int\",\n",
        "              \"garment_group_no\":\"int\"}\n",
        "cust_dtypes = {\"customer_id\":\"str\"}\n",
        "\n",
        "obj = \"class\" # \"class\" or \"rank\"\n",
        "N = 15000\n",
        "n_iter = 2 # num of iteration\n",
        "idx_file = \"exp1\"\n",
        "n_round = 2000\n",
        "n_splits = 1\n",
        "nobuy = 20 # num of negative samples"
      ],
      "id": "21e76b18-8d9e-4a1b-bf84-a929c8f16c6f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3df9c2e0-af9e-406d-a9c6-c38a87347728"
      },
      "source": [
        "While we vary the user represnetations, we will keep the article representation fixed. The code below reads the article.csv file and extracts a number of features to represent articles."
      ],
      "id": "3df9c2e0-af9e-406d-a9c6-c38a87347728"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb93a7cd-a637-4960-9577-08449a57b55d",
        "outputId": "25c2720f-56fe-48f4-9752-97e038969d50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:598: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:598: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
            "  FutureWarning,\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(\"hmdata/articles.csv.zip\")\n",
        "\n",
        "## Find categorical columns\n",
        "ohe_columns = []\n",
        "total = 0\n",
        "\n",
        "for col in df.columns:\n",
        "    if df[col].dtype == \"int64\" and len(df[col].unique()) <= 500:\n",
        "        ohe_columns.append(col)\n",
        "        total += len(df[col].unique())\n",
        "        \n",
        "## Do one hot encoding of the above categorical variables\n",
        "V = pd.get_dummies(df[ohe_columns], columns=ohe_columns).values\n",
        "\n",
        "\n",
        "## Get article features\n",
        "tfidf = TfidfVectorizer(min_df=3)\n",
        "V_desc = tfidf.fit_transform(df[\"detail_desc\"].fillna(\"nodesc\"))\n",
        "\n",
        "## Represent articles as vector of size 512\n",
        "EMB_SIZE = 128 #note -- reducing to avoid memory issues...\n",
        "V = np.hstack([V.astype(\"float32\"), V_desc.todense()])\n",
        "svd = TruncatedSVD(n_components=EMB_SIZE, random_state=0)\n",
        "svd.fit(V)\n",
        "V = svd.transform(V)\n",
        "\n",
        "np.save(\"articles.npy\", V)"
      ],
      "id": "fb93a7cd-a637-4960-9577-08449a57b55d"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "64a3715e-de70-49d4-b057-e0b674ca65ff"
      },
      "outputs": [],
      "source": [
        "def item_representation_1():\n",
        "    df_art = pd.read_csv(path+\"articles.csv.zip\",dtype=art_dtypes)\n",
        "    le = LabelEncoder()\n",
        "    le.fit(df_art[\"index_code\"].unique())\n",
        "    df_art[\"index_code\"] = le.transform(df_art[\"index_code\"])\n",
        "    \n",
        "    dict_vec = {}\n",
        "    vec_art = np.load(\"articles.npy\")\n",
        "    df_vec = pd.concat([df_art[\"article_id\"],pd.DataFrame(vec_art)],axis=1)\n",
        "    for i in range(len(vec_art)):\n",
        "        dict_vec[df_art[\"article_id\"][i]] = vec_art[i]\n",
        "    del vec_art,df_vec\n",
        "    \n",
        "    return df_art, dict_vec\n",
        "    "
      ],
      "id": "64a3715e-de70-49d4-b057-e0b674ca65ff"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cbe5f64-354c-451e-a9e0-eb114440f8f0"
      },
      "source": [
        "Taken together, the two cells above give us all the features we want to represent articles."
      ],
      "id": "7cbe5f64-354c-451e-a9e0-eb114440f8f0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0c3082e-1da7-466f-8b9f-c777c8ef63cd"
      },
      "source": [
        "Now lets define some functions to extract user representations. The different functions will contain different ways of representing users.\n",
        "\n",
        "We bootstrap by providing a simple set of features to represent users in user_representation_1(). This function returns the dataframe of user features."
      ],
      "id": "f0c3082e-1da7-466f-8b9f-c777c8ef63cd"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "5de63a95-ef6c-4796-8e59-0cb33b9b948e"
      },
      "outputs": [],
      "source": [
        "def user_representation_1():\n",
        "    df_cust = pd.read_csv(path+\"customers.csv.zip\",dtype=cust_dtypes)\n",
        "    df_cust[\"age\"] = df_cust[\"age\"].fillna(df_cust[\"age\"].mean())\n",
        "    df_cust[[\"FN\",\"Active\"]] = df_cust[[\"FN\",\"Active\"]].fillna(0)\n",
        "    df_cust[\"club_member_status\"] = df_cust[\"club_member_status\"].apply(lambda x:1 if x == \"ACTIVE\" else 0)\n",
        "    df_cust[\"fashion_news_frequency\"] = df_cust[\"fashion_news_frequency\"].apply(lambda x:0 if x == \"NONE\" else 1)\n",
        "    df_cust = df_cust.drop([\"postal_code\"], axis=1)\n",
        "    return df_cust"
      ],
      "id": "5de63a95-ef6c-4796-8e59-0cb33b9b948e"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "65dbcef5-701a-4999-b696-3d871ccb485d"
      },
      "outputs": [],
      "source": [
        "def user_representation_2():\n",
        "    \"\"\"\n",
        "    TODO -- compute user representations as the average\n",
        "    of the embeddings of the recently purchased articles\n",
        "    return user representation\n",
        "\n",
        "    Hint: You may find pd.DataFrame(item_representation_1()[1]).transpose() useful\n",
        "    \"\"\"\n",
        "    #creating user vector as mean of item vector which a user purchad in a session  -- session vector \n",
        "    day_start = datetime.datetime(2019,9, 23) - datetime.timedelta(days=36)\n",
        "    day_end = datetime.datetime(2019, 9, 23) - datetime.timedelta(days=6)\n",
        "\n",
        "    df_trans = pd.read_csv(\"hmdata/transactions_train.csv.zip\")\n",
        "    df_trans.t_dat = pd.to_datetime(df_trans.t_dat)\n",
        "\n",
        "    df_trans = df_trans[(df_trans.t_dat > day_start) & (df_trans.t_dat < day_end)]\n",
        "\n",
        "    article_embedings = pd.DataFrame(item_representation_1()[1]).transpose().reset_index()\n",
        "    article_embedings.columns = [\"article_id\"] + [f\"embed_{i}\" for i in range(len(article_embedings.columns.to_list())-1)]\n",
        "\n",
        "    df_merged = df_trans.merge(article_embedings, how=\"left\", on=\"article_id\").sort_values(by=\"t_dat\")\n",
        "\n",
        "    df_merged = (df_merged.groupby([\"customer_id\"])\n",
        "                .tail(5)[[\"customer_id\"] + [f\"embed_{i}\" for i in range(len(article_embedings.columns.to_list())-1)]]\n",
        "                .groupby(\"customer_id\")\n",
        "                .mean()\n",
        "                .reset_index())\n",
        "    \n",
        "    del df_trans\n",
        "    del article_embedings\n",
        "\n",
        "    gc.collect()\n",
        "\n",
        "    return df_merged\n",
        "    \n",
        "\n",
        "    return None\n",
        "\n",
        "def user_representation_3():\n",
        "    \"\"\"\n",
        "    OPTIONAL -- compute user representations as the output\n",
        "    of the doc2vec model.\n",
        "    https://cs.stanford.edu/~quocle/paragraph_vector.pdf\n",
        "    Doc2vec model is an embedding learning method\n",
        "    that enables us to learn representations of a document.\n",
        "    We treat each user as a document, and the set of articles\n",
        "    the user has purchased as the set of words in the document.\n",
        "    \"\"\"\n",
        "    return None"
      ],
      "id": "65dbcef5-701a-4999-b696-3d871ccb485d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d39b7430-bf43-4f16-b7a8-a6edbd078e1d"
      },
      "source": [
        "As part of the goal for part B of this week's project, please use the above two functions to implement the two user representation techniques mentioned in the project jumpstart.\n",
        "\n",
        "You can run the rest of the notebook for now, and come back to these functions, implement them and re-run some of the code below and use user_representation_2() (and optionally user_representation_3()) to get the appropriate user features to use to train the model for the downstream task.\n",
        "\n",
        "Lets write a function that would read the transactions data and return the dataframes for the transactions within the dates we want to consider, along with the dataframes for articles features: df_art and dict_vec."
      ],
      "id": "d39b7430-bf43-4f16-b7a8-a6edbd078e1d"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "55c3e4dd-ce84-4199-9215-06490305db3c"
      },
      "outputs": [],
      "source": [
        "path = \"hmdata/\"\n",
        "def read_data(day_oldest):\n",
        "    df_trans = pd.read_csv(path+\"transactions_train.csv.zip\",dtype=tran_dtypes)\n",
        "    df_trans[\"t_dat\"] = pd.to_datetime(df_trans[\"t_dat\"],format=\"%Y-%m-%d\")\n",
        "\n",
        "    df_trans = df_trans.query(f\"t_dat >= '{day_oldest}'\").copy()\n",
        "    df_trans = df_trans.drop_duplicates([\"customer_id\",\"article_id\",\"t_dat\"])\n",
        "    df_art,dict_vec = item_representation_1()\n",
        "    df_trans = df_trans.merge(df_art[[\"article_id\",\"product_code\",\"product_type_no\",\"graphical_appearance_no\",\"colour_group_code\",\"department_no\",\"index_code\",\"index_group_no\",\"section_no\",\"garment_group_no\"]],how=\"left\",on=\"article_id\")\n",
        "\n",
        "    return df_trans, df_art, dict_vec"
      ],
      "id": "55c3e4dd-ce84-4199-9215-06490305db3c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4128aa41-0a7c-405c-a2c2-02a3e19678f2"
      },
      "source": [
        "Now we have all the ingredients we need -- we have a basic version of user representations and we have the article representations, and transactions data on which we can train our downstream task.\n",
        "\n",
        "The downstream task we consider is the task of predicting whether or not a user will purchase an article. This is the same task that we have been dealing with in the past 2 weeks.\n",
        "\n",
        "Lets define a train() function that will consider the start and end dates and split data based on these, generate the training data, do random negative sampling and train the model."
      ],
      "id": "4128aa41-0a7c-405c-a2c2-02a3e19678f2"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "5e0c7a4f-8e4d-42f7-b379-1ebc870375d9"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "    #### Transaction start date say it is from 2019/9/23 and say we take 1 week data\n",
        "    day_start = datetime.datetime(2019,9,23) - datetime.timedelta(days=6)\n",
        "    #### Transaction end date\n",
        "    day_end = datetime.datetime(2019,9,23) - datetime.timedelta(days=0)\n",
        "    \n",
        "    ######## Splitting data based on date ###########################\n",
        "    ####### Train date ###########################################\n",
        "    ## Let's consider the training data for 1 year\n",
        "    day_start_hist = day_start - datetime.timedelta(days=366)\n",
        "    day_end_hist = day_start - datetime.timedelta(days=1)\n",
        "    \n",
        "    df_trans, df_art, dict_vec = read_data(day_oldest = datetime.datetime(2018,9,23))\n",
        "\n",
        "    df_cust1 = user_representation_1()\n",
        "    df_cust2 = user_representation_2()\n",
        "\n",
        "\n",
        "    query_date = f\"((t_dat >= '{day_start}') and (t_dat <= '{day_end}'))\"\n",
        "    top_art_all = df_trans.query(query_date ).groupby(\"article_id\")[\"t_dat\"].count().sort_values(ascending = False).index[:N].tolist()\n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "    ############### Create training data #################################################################################\n",
        "    \n",
        "    \n",
        "    list_df_buy = []\n",
        "    list_cust =[]\n",
        "    \n",
        "    # make positive samples\n",
        "    list_df_buy = df_trans.query(f\"(t_dat >= '{day_start}') and (t_dat <= '{day_end}') and (article_id in @top_art_all)\").drop_duplicates([\"customer_id\",\"article_id\"])[[\"customer_id\",\"article_id\"]].copy()\n",
        "    list_df_buy[\"target\"] = 1\n",
        "    list_cust = list_df_buy[\"customer_id\"].unique().tolist()\n",
        "        \n",
        "        \n",
        "    # make negative samples (random selection)\n",
        "    \n",
        "    list_df_nobuy = pd.concat([pd.DataFrame({\"customer_id\":x,\"article_id\":random.sample(top_art_all,nobuy)}) for x in list_cust])\n",
        "    list_df_nobuy[\"target\"] = 0\n",
        "    list_train = pd.concat([list_df_buy,list_df_nobuy]).drop_duplicates([\"customer_id\",\"article_id\"])\n",
        "    del list_df_nobuy\n",
        "\n",
        "    # add feature\n",
        "    df_train = pd.DataFrame()\n",
        "    \n",
        "    ########## Merging item features with the transactions data ###################################################\n",
        "    list_train = list_train.merge(df_art[[\"article_id\",\"product_code\",\"product_type_no\",\"graphical_appearance_no\",\"colour_group_code\",\"department_no\",\"index_code\",\"index_group_no\",\"section_no\",\"garment_group_no\"]],how=\"left\",on=\"article_id\")\n",
        "    \n",
        "    ######### Merging customer data with the above data ######################################\n",
        "    list_train = list_train.merge(df_cust1, how=\"left\", on=\"customer_id\")\n",
        "    list_train = list_train.merge(df_cust2, how=\"left\", on=\"customer_id\")\n",
        "    df_train = df_train.append(list_train)\n",
        "    del list_train\n",
        "    gc.collect()\n",
        "    \n",
        "    \n",
        "    # now that we have all the data in place, lets train the lgbm model\n",
        "\n",
        "    # train lgbm\n",
        "    X_train = df_train.drop([\"customer_id\",\"product_code\",\"product_type_no\",\"department_no\",\"target\"],axis=1)\n",
        "    y_train = df_train[\"target\"]\n",
        "    del df_train\n",
        "    \n",
        "    X_tr, X_va, y_tr, y_va = train_test_split(X_train,y_train,stratify = y_train)\n",
        "    d_tr = lgb.Dataset(X_tr, label=y_tr,  free_raw_data=False)\n",
        "    d_va = lgb.Dataset(X_va, label=y_va,  free_raw_data=False)\n",
        "    lgbm_model = lgb.train(lgb_params, train_set=d_tr, num_boost_round=n_round, valid_sets=[d_tr,d_va], verbose_eval=500, early_stopping_rounds=100)\n",
        "    \n",
        "    # save model\n",
        "    pd.to_pickle(lgbm_model,\"lgbm_model.pkl\")\n",
        "    del X_train, y_train, X_tr, X_va, y_tr, y_va, d_tr, d_va\n",
        "    gc.collect()\n",
        "    del df_trans, df_art, df_cust\n",
        "    gc.collect()\n",
        "    return 0"
      ],
      "id": "5e0c7a4f-8e4d-42f7-b379-1ebc870375d9"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "afb1b72d-7f50-4dc8-a44c-95dcf9c537d2",
        "outputId": "b82466b4-de52-49ba-d33d-12d54d1afed9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 100 rounds.\n",
            "[500]\ttraining's auc: 0.818213\tvalid_1's auc: 0.805443\n",
            "[1000]\ttraining's auc: 0.839239\tvalid_1's auc: 0.815591\n",
            "[1500]\ttraining's auc: 0.853173\tvalid_1's auc: 0.821016\n",
            "[2000]\ttraining's auc: 0.864274\tvalid_1's auc: 0.825187\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[2000]\ttraining's auc: 0.864274\tvalid_1's auc: 0.825187\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-01ba1bfbc4d7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_va\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_va\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_va\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0;32mdel\u001b[0m \u001b[0mdf_trans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_art\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_cust\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'df_cust' referenced before assignment"
          ]
        }
      ],
      "source": [
        "train()"
      ],
      "id": "afb1b72d-7f50-4dc8-a44c-95dcf9c537d2"
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "CYGdCNkGu1DN"
      },
      "id": "CYGdCNkGu1DN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "Run1 - using user_representation_1()\n",
        "Training until validation scores don't improve for 100 rounds.\n",
        "[500]\ttraining's auc: 0.810789\tvalid_1's auc: 0.804822\n",
        "[1000]\ttraining's auc: 0.824949\tvalid_1's auc: 0.81536\n",
        "```\n",
        "\n",
        "```\n",
        "Run2 - using user_representation_2(). ----- using 128dim SVD --- running into  memory for 512\n",
        "Training until validation scores don't improve for 100 rounds.\n",
        "[500]\ttraining's auc: 0.811333\tvalid_1's auc: 0.798351\n",
        "[1000]\ttraining's auc: 0.832394\tvalid_1's auc: 0.808383\n",
        "[1500]\ttraining's auc: 0.846529\tvalid_1's auc: 0.813824\n",
        "[2000]\ttraining's auc: 0.857308\tvalid_1's auc: 0.817434\n",
        "Did not meet early stopping. Best iteration is:\n",
        "[2000]\ttraining's auc: 0.857308\tvalid_1's auc: 0.817434\n",
        "0\n",
        "```\n",
        "\n",
        "```\n",
        "Run3 - using user_representation_1() and user_representation_2()\n",
        "Training until validation scores don't improve for 100 rounds.\n",
        "[500]\ttraining's auc: 0.818213\tvalid_1's auc: 0.805443\n",
        "[1000]\ttraining's auc: 0.839239\tvalid_1's auc: 0.815591\n",
        "[1500]\ttraining's auc: 0.853173\tvalid_1's auc: 0.821016\n",
        "[2000]\ttraining's auc: 0.864274\tvalid_1's auc: 0.825187\n",
        "Did not meet early stopping. Best iteration is:\n",
        "[2000]\ttraining's auc: 0.864274\tvalid_1's auc: 0.825187\n",
        "```"
      ],
      "metadata": {
        "id": "Q1W2Ga4Su2xO"
      },
      "id": "Q1W2Ga4Su2xO"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4940f48d-08e5-4130-be2c-d15622012620"
      },
      "source": [
        "Once you have implemented the function, please note to change the line:\n",
        "\n",
        "df_cust = user_representation_1()\n",
        "\n",
        "to the appropriate function name and run re-train the model. Please report the performance numbers with each of the two user representations.\n",
        "\n",
        "This should complete the week 3 project!"
      ],
      "id": "4940f48d-08e5-4130-be2c-d15622012620"
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "- user-vector = session vector as feature did give a boost in validation accuracy\n",
        "- gains can be seen if we augment this with our original set of features.\n",
        "```"
      ],
      "metadata": {
        "id": "LxPTemGW6DIW"
      },
      "id": "LxPTemGW6DIW"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydLW7dPiuw-E"
      },
      "source": [
        "### Optional task 1: training a Doc2Vec model\n",
        "\n",
        "If you want an extra challenge, you can try implementing Doc2vec representations in user_representation_3(). The Doc2vec model is an embedding learning method\n",
        "    that enables us to learn representations of a document.\n",
        "    We treat each user as a document, and the set of articles\n",
        "    the user has purchased as the set of words in the document."
      ],
      "id": "ydLW7dPiuw-E"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce141145-f5af-4f2d-9d94-f4bb50e6208f"
      },
      "source": [
        "### Optional task 2: training a sequential LSTM model\n",
        "\n",
        "Another optional task here would be to implement user_representation_4() where user representations are learnt by a sequential LSTM model. The LSTM model will need to be trained on a task -- the task itself could be the downstream task of predicting whether or not a user would purchase a given article given a sequence of previous articles. The final hidden layer of the lstm model can be used as the user representation."
      ],
      "id": "ce141145-f5af-4f2d-9d94-f4bb50e6208f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88eb3b58-062c-4c96-b3f0-2db4407ce1b3"
      },
      "outputs": [],
      "source": [
        ""
      ],
      "id": "88eb3b58-062c-4c96-b3f0-2db4407ce1b3"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "name": "Copy of week3-ann-user-representations.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "environment": {
      "kernel": "python3",
      "name": "tf2-gpu.2-6.m89",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m89"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}